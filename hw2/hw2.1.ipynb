{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ДЗ 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Выбрать и сохранить книгу в .txt\n",
    "Выбрана книга \"Белая гвардия\", сохранена в txt в кодировке utf-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Обработать книгу с помощью mystem:\n",
    "##### 2.1. Распарсить с помощью mystem\n",
    "##### 2.2. Замерить время работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "file_name = \"white_guardian.txt\"\n",
    "with open(file_name, encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим знаки пунктуации с помощью функции translate, транслируя каждый символ из пунктуации в ничто. Символы конца строки превратим в пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct_chars = string.punctuation\n",
    "text = text.translate({ord(ch): None for ch in punct_chars})\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем и замерим время:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analysis = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Сохранить результат в json\n",
    "В прошлый раз использовали load() для импорта, теперь используем dump() для экспорта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('mystem_analysis.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Обработать книгу через pymorphy\n",
    "##### 3.1. Токенизировать текст с помощью nltk\n",
    "##### 3.2. Замерить время работы\n",
    "Импортируем word_tokenize. Здесь может появиться ошибка типа [этой](https://github.com/joosthub/PyTorchNLPBook/issues/14), поступаем, как сказано в ответах к ишью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_text = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим экземпляр класса MorphAnalyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отпарсим все слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_words = []\n",
    "\n",
    "for word in tokenized_text:\n",
    "    parsed_word = morph.parse(word)\n",
    "    parsed_words.append(parsed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3, 3.4. Сохранить результат (хотя бы лемма + часть речи) в json\n",
    "Создадим список, куда свалим все разборы. А сами разборы преобразуем в словари по тэгам \"в лоб\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_parsing = []\n",
    "\n",
    "for word in parsed_words:\n",
    "    w = word[0]\n",
    "    entry = {\n",
    "        'word': w.normal_form,\n",
    "        'pos': w.tag.POS,  # Part of Speech, часть речи\n",
    "        'animacy': w.tag.animacy,  # одушевленность\n",
    "        'aspect': w.tag.aspect,  # вид: совершенный или несовершенный\n",
    "        'case': w.tag.case,  # падеж\n",
    "        'gender': w.tag.gender,  # род (мужской, женский, средний)\n",
    "        'involvement': w.tag.involvement,  # включенность говорящего в действие\n",
    "        'mood': w.tag.mood,  # наклонение (повелительное, изъявительное)\n",
    "        'number': w.tag.number,  # число (единственное, множественное)\n",
    "        'person': w.tag.person,  # лицо (1, 2, 3)\n",
    "        'tense': w.tag.tense,  # время (настоящее, прошедшее, будущее)\n",
    "        'transitivity': w.tag.transitivity,  # переходность\n",
    "        'voice': w.tag.voice,  # залог (действительный, страдательный)\n",
    "    }\n",
    "    list_of_parsing.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложим в json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"list_of_parsing.json\"\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump(list_of_parsing, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ответить на вопросы:\n",
    "##### 4.1. Какую долю слов составляет каждая часть речи? (Например, для глагола - это количество глаголов, деленное на общее число слов в тексте)\n",
    "Сложим части речи в отдельный список и посчитаем все вхождения с помощью Counter, заодно отсортировав их для пущей наглядности с помощью most_common()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN 29.973306215838313\n",
      "VERB 14.013530500120053\n",
      "ADJF 11.390760278519272\n",
      "PREP 10.882307246868072\n",
      "CONJ 9.954380464104629\n",
      "ADVB 6.025168425066735\n",
      "NPRO 5.276612572913577\n",
      "PRCL 4.320438399502846\n",
      "INFN 1.6468228747369462\n",
      "PRTF 1.2513594056749007\n",
      "GRND 1.1976893634450516\n",
      "ADJS 1.0734008445969805\n",
      "NUMR 0.7965764162535485\n",
      "None 0.5508241176221346\n",
      "PRED 0.48726748866573455\n",
      "PRTS 0.47314379334209\n",
      "INTJ 0.4011129471915032\n",
      "COMP 0.28529864553761847\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "parts_of_speech = []\n",
    "\n",
    "for parsing in list_of_parsing:\n",
    "    parts_of_speech.append(parsing['pos'])\n",
    "\n",
    "pos_count = Counter(parts_of_speech).most_common()\n",
    "total_count = len(parts_of_speech)\n",
    "\n",
    "for k, v in pos_count:\n",
    "    print(k, v/total_count*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Найдите топ-20 (по частотности) глаголов и наречий\n",
    "Снова используем тот же подход к аггрегации и сортировке, а затем выведем топ 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 verbs:\n",
      "\n",
      "быть 669\n",
      "сказать 204\n",
      "стать 165\n",
      "мочь 152\n",
      "говорить 126\n",
      "идти 99\n",
      "ответить 94\n",
      "знать 91\n",
      "спросить 63\n",
      "видеть 56\n",
      "выйти 55\n",
      "думать 54\n",
      "дать 51\n",
      "бежать 45\n",
      "пройти 45\n",
      "исчезнуть 42\n",
      "сидеть 42\n",
      "начать 41\n",
      "ходить 41\n",
      "подумать 41\n",
      "\n",
      "Top 20 adverbs:\n",
      "\n",
      "уже 140\n",
      "очень 102\n",
      "тут 99\n",
      "где 97\n",
      "там 95\n",
      "потом 91\n",
      "сейчас 87\n",
      "совершенно 81\n",
      "потому 71\n",
      "вдруг 68\n",
      "теперь 58\n",
      "здесь 50\n",
      "затем 50\n",
      "опять 46\n",
      "уж 42\n",
      "тогда 40\n",
      "совсем 38\n",
      "сразу 38\n",
      "сегодня 32\n",
      "несколько 32\n"
     ]
    }
   ],
   "source": [
    "list_of_verbs = []\n",
    "list_of_adverbs = []\n",
    "\n",
    "for parsing in list_of_parsing:\n",
    "    if 'VERB' == parsing['pos']:\n",
    "        list_of_verbs.append(parsing['word'])\n",
    "    if 'ADVB' == parsing['pos']:\n",
    "        list_of_adverbs.append(parsing['word'])\n",
    "\n",
    "verbs_count = Counter(list_of_verbs).most_common()\n",
    "adverbs_count = Counter(list_of_adverbs).most_common()\n",
    "\n",
    "print(\"Top 20 verbs:\\n\")\n",
    "for k, v in verbs_count[:20]:\n",
    "    print(k, v)\n",
    "\n",
    "print(\"\\nTop 20 adverbs:\\n\")\n",
    "for k, v in adverbs_count[:20]:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Посмотрите документацию для nltk н-грамм (nltk.bigrams, например), попробуйте составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания). Почему получаются именно такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложим все нормальные формы в список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = []\n",
    "for parsing in list_of_parsing:\n",
    "    lemma_list.append(parsing['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем модули и считаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('и', 'в') 102\n",
      "('не', 'быть') 82\n",
      "('в', 'город') 76\n",
      "('и', 'не') 68\n",
      "('что', 'он') 66\n",
      "('потому', 'что') 65\n",
      "('не', 'мочь') 56\n",
      "('у', 'он') 51\n",
      "('мочь', 'быть') 50\n",
      "('он', 'и') 49\n",
      "('господин', 'полковник') 48\n",
      "('он', 'не') 45\n",
      "('не', 'знать') 43\n",
      "('он', 'в') 43\n",
      "('он', 'быть') 41\n",
      "('что', 'вы') 40\n",
      "('ничто', 'не') 39\n",
      "('николка', 'и') 38\n",
      "('и', 'на') 36\n",
      "('и', 'весь') 36\n",
      "('в', 'глаз') 34\n",
      "('и', 'он') 33\n",
      "('тот', 'что') 33\n",
      "('в', 'чёрный') 32\n",
      "('в', 'столовый') 32\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "bigram_list = Counter(list(bigrams(lemma_list))).most_common()\n",
    "\n",
    "for bigram in bigram_list[:25]:\n",
    "    print(bigram[0], bigram[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('что', 'же', 'это') 14\n",
      "('у', 'он', 'быть') 12\n",
      "('ни', 'в', 'кой') 11\n",
      "('в', 'кой', 'случай') 11\n",
      "('в', 'город', 'и') 11\n",
      "('не', 'мочь', 'быть') 10\n",
      "('не', 'знать', 'что') 9\n",
      "('в', 'это', 'время') 9\n",
      "('в', 'тот', 'же') 9\n",
      "('черта', 'он', 'знать') 8\n",
      "('о', 'тот', 'что') 8\n",
      "('до', 'сей', 'пора') 7\n",
      "('в', 'три', 'час') 7\n",
      "('я', 'думать', 'что') 7\n",
      "('до', 'тот', 'пора') 7\n",
      "('и', 'не', 'быть') 7\n",
      "('в', 'конец', 'конец') 7\n",
      "('один', 'из', 'они') 7\n",
      "('слушать', 'господин', 'полковник') 7\n",
      "('у', 'он', 'на') 7\n",
      "('и', 'сейчас', 'же') 7\n",
      "('в', 'самый', 'дело') 6\n",
      "('в', 'всякий', 'случай') 6\n",
      "('мочь', 'быть', 'и') 6\n",
      "('кой', 'случай', 'не') 6\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import trigrams\n",
    "trigram_list = Counter(list(trigrams(lemma_list))).most_common()\n",
    "\n",
    "for trigram in trigram_list[:25]:\n",
    "    print(trigram[0], trigram[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь любопытны:\n",
    "* триграмма \"что же это\", указывающая на невероятность и трагичность описываемых в книге событий;\n",
    "* биграммы и триграммы со словами \"город\", косвенно намекающие, что город - квази-персонаж романа;\n",
    "* \"господин полковник\" и \"слушать господин полковник\" - результат, по-видимому, намеренного употребления этого выражения по отношению к герою романа полковнику Малышеву (это, возможно, оттого, что это реальное лицо под своей фамилией)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
